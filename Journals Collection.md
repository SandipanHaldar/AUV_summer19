# Washington State University
A simulation  platform  wasdeveloped  using  the  Unity  Engine  for  verification  testing  toensure functionality before time sensitive testing within a realpool.
The bot used  four  depth  sensors,  four  IMUs  each  con-sisting of a 3-axis gyroscope, accelerometer, and magnetometer, four hydrophones, two front-facing stereoscopic cameras,one  bottom-facing  camera,  and  current,  voltage,  and  rpmsensors  embedded  in  each  of  the  8  thrusters.  All  sensor  datais  integrated  using  an  Unscented  Kalman  Filter  (UKF),  with a particle  filter  acting  as  an  intermediary  for  the  hydrophone and camera data.
Gyroscopes, Depth sensors, Embedded Thruster sensor, Accelerometer, Magnetometer are notable instruments used. 
The magnetometers targeted IIR notch filters have been ex-perimented with successfully to remove oscillations, a simpleFIR Low pass filter is sufficient, and provides a consistent timedelay  which  is  more  preferable. 
Now comes the stimulator part
* Sensors- Hardware  sensors  were  not  simulated  directly due to their complexity, but their outputs were matched using Unity’s  built  in  3D  transforms.  Every  loop,  the  simulatedsensor module calculates the orientation of the submarine and outputs it to the software for processing. Noise can be added to  more  closely  simulate  bad  sensor  readings  depending  on observed noise within the real system.
* Thrusters:Thrusters  were  also  not  simulated  directly since fluid dynamics are too computationally expensive. However,  since  thrusters  were  linearized  in  the  real  system,  it  is assumed linear scaling of thruster values to the engine, making thruster simulation trivial. The thrusters are treated as a list of thrust  vectors  loaded  from  the  submarine  configuration  file.This  loads  the  offsets  of  the  thrusters  to  the  center  of  mass,their  orientations,  and  their  minimum  and  maximum  thrust.
* Cameras:Cameras were also not simulated directly due to  the  complexities  of  fish-eye  projection  and  lens  distortion.Fish-eye projection is also not supported in OpenGL or Unityand  must  be  done  manually  using  multiple  rectilinear  view-ports.  To  simplify,  since  cameras  are  calibrated,  undistorted,and  rectified  for  the  actual  system,  the  extrinsics  can  beloaded  to  place  cameras  in  the  same  location  and  projectionspace as the real submarine to recover the rectified coordinateframe. The vision pipeline then becomes identical as the realone barring re-projection errors within the original calibrationroutine.
* Submarine:Simulating the actual physics of the submarine was done much more directly than any other portion of the simulation. Since the Unity engine supports rigid body physics,mesh  rendering,  and  drag,  most  of  the  work  was  importing these  components  into  the  engine  using  approximations  from the original mechanical engineering designs. A visually representative rendering was done using a down-sampled mesh and texture  overlays  from  the  SolidWorks  model.  Statics  such  as the center of drag, mass, and buoyancy were also taken using SolidWorks  calculations  based  on  the  estimated  properties  of the submarine.  A simple  tensor  rotational/translational  drag  and  inertial  modelwas  made  that  could  closely  mimic  the  dynamics  of  the submarine in water without the complexities of fluid dynamics.
* Pool:Since the submarine can be tested in many different environments,  the  pool  simulation  is  just  as  important  as the  actual  submarine.  Pool  and  obstacle  simulation,  water reflections, fog rendering, lighting are all very complex problems  to  simulate  properly.  The  competition  pool  was  made using  Google  Earth  measurements  along  with  documented measurements of the geometry of the bottom. Obstacles were made  from  documents  as  well  and  can  be  placed  and  moved as needed once the final layout is measured at competition. All models were made using Google SketchUp and were imported to  the  engine  using  a  common  format.  Collision  meshes  and other scripted elements were generated once imported to allow entity interaction and debugging. To simulate water, the built-in  Unity  water  shaders  were  used  along  with  an  exponential fog  layer  in  the  camera  rendering  pipeline.  Lighting  is  also built  into  the  engine  and  can  be  adjusted  depending  the  timeof day and lighting conditions.  
# Utah State university
The microcomputer and microcontroller is based on a linux operating system
One Arduino Duo, a basic micro controller is present. 
Also utilizes four _Beaglebone Blacks_:microcontrollers similar to Raspberry Pi but more suited for robotics.
Raspberry Pi image capturing abilities were more user friendly so the replaced the three Beaglebones with one Raspberry Pi.
For vision they used opencv libary and coded in python.
Linux Debianis the ideal choice for Beaglebone Blacks due to an abundance of community support and available software packages. Linux Debian is also straightforward in installation and simple to use. 
# University if Victoria
* The  data from the  IMU and the  depth sensor are used to calculate  the  current orientation of the  vehicle. This data  is compared   to   the   location   of   the   target   object   and   a trajectory  to  the  target  object  is  calculated.  To  follow  this trajectory each motor needs to be set to a certain speed and the  computer  calculates  the  appropriate  speed  for  each motor and sends the commands.
* The computer of the AUV uses Linux mint running ROS.  ROS  nodes  and  nodelets  are used  to  control  the  sub. The  computer  sends  commands with an address for the specific peripheral. The junction box receives  this  command  and  reads  the  address  to  figure  out whom it is addressed to. It then converts the message to the right  protocol  for  the  intended  recipient  and  sends  the message to the peripheral. If the message is addressed to the junction  box  itself,  it  reads  the  message  and  carries  out  its commands.
Vision processing is done with OpenCV, which integrates with ROS. The vision is broken down into four parts
* Objects are recognised from a combination of edge detection and and colour recognition.   To  recognize  a specific  object,  a  classifier  is  created for  that  object  an trained to recognize the object by using several images with the object present as well as images of the background with the  object  not  present. A GUI was created to adjust the colour and edges for training the classifier.  The    classifier    also    does    not recognize an object when it is rotated, so the frames need to be  looked  at  four  times,  once  in  each  orientation,  to  see  if the classifier matches the object.
* Most of the  peripherals have  two set  of software. One  is embedded  code  on  a  microcontroller  to  control  the  actual operation  of  the  peripheral.  The  microcontroller  reads  the messages from the main computer and turns these messages into PWM signals that operate the devices. The peripherals get  their  instructions  from  the  task  routines  in  the  main control software.
# University of Toronto
Tempest’s software is organized in a Model-View-Controller (MVC) framework built upon the OpenCV and Qt libraries. Coded in C++, it is designed to be adaptable and user-friendly using Agile development principles. Tempest also includes a custom simulator to allow for virtual testing of the software algorithms. 
The  MVC  architecture  allows  for  more  modular code. The modularity decreases the learning curve for new  developersas  existing  code  can  be  treated  as black   boxes. 
The  task  algorithms  are  designed  to  control the movement   of   the   submarine.   They   receive   video feeds  from  the  front  and  bottom  cameras  as  well  as data from the FPGA. The video passes through a hue saturation  value  (HSV)  filter,  which  isolates  features that  are  within  preset  color  thresholds.  The  filtered video is then passed to other filters to undergo shape recognition.
The task algorithms use the geometry and position of  the  detected  shapes  as  well  as  the  data  from  the FPGA to execute motor commands. 
Tempest’s software testing system incorporates an in-house logger which allows for customizable output for  easy  debugging,  and  a  simulator  which  creates  a virtual environment to test the task algorithms.
The  logger  can  be  treated  as  an  augmented  print statement with     additional     features     such     as timestamps,   logging   levels,   and   writing   to   file. Logging also helps with code readability, which is an industry expectation.
The simulator is built using the 3D game engine Irrlicht. Irrlicht is chosen due to its    C++    libraries,    allowing    for    straightforward integration with the rest of the Tempest code. Irrlicht is  also  lightweight,  allowing  it  to  run  fairly  fast  on less powerful laptops.
# University of Southern California
* Ubuntu Linux 10.10 was used considering its open source nature
* ROS was used to operate the system and system  components  are discretised  into  units  called “nodes”,  each  of  which  has  at  least  one  dedicated thread and the ability to control parts of its life cycle. 
When     necessary,     these     nodes     are     able     to communicate    via    explicitly    defined,    language-generic   messages   sent   over   a   named,   simplex channel, or “topic”. The direction of these topics is determined at compile time; a node can “advertise” an  outgoing  topic  via  a  “publisher”  object  or “subscribe” to an incoming topic via a “subscriber” object.    However,    topics    can    be    redirected    or “remapped” at runtime, allowing for the creation of more loosely-defined distributed systems
More complex paradigms, such as “actions” or “preemptable tasks”, have been implemented to take advantage of this fact. 
 ROS   provides   fairly generic implementations of many common algorithms  known  to  the  field  of  robotics,  including such categories as sensing,  navigation, planning, and visualization.
 * The  serialization-free communication  described  above  is  traditionally  time consuming  to  set  up,  as  modules  utilizing  it  must follow the “nodelet” paradigm rather than the more common “node” paradigm. This  issue  is  resolvedby maintaining  a  set  of  scripts  and  generic  wrappers around the more commonly-used ROS components in an  open-source package called “quickdev” available in USC's “usc-ros-pkg”.
 * Vision - Seabee  views  the  world  through two   PointGreyFirefly   USB   cameras:   one   facing forward and one facing downward. Both cameras are configured  to  stream  bayered  320x240  images  at  30 hz. 
 The raw image captured had was distorted due to the surrounding materials and hazzy due to water.  To  compensate  for  these effects, anotheruseful  community-developed  ROS nodeis  usedcalled  “image_proc”,  which  utilizes several  common  OpenCV  functions  to  de-bayer  and un-distort  (given  a  camera  calibration)  the  incoming raw images.
 * In order  to  optimize the  vision  algorithms  farther  down the   pipeline, the   codeseeksto   mimic   neural adaptation  and  avoid  unnecessarily  processing  pixels that  are  not  changing  by  a  sufficient  amount.  This value is determined by calculating a weighted sum of the differences between each pixel in each channel in the  current  image  and  the  value  of  that  pixel  at  the point  in  time  that  it  was  last  determined  to  have changed.   Pixels   found   to   have   changed   by   a minimum  amount  are  recorded  in  a  binary  mask, which is published along with the corresponding HSL image. This mask-image pair is called an “adaptation image.” 
* Seabee    utilizes    an    extensible landmark  recognizer  that  accepts  a  landmark  filter and  calls  on  child  modules  to  perform  specialized landmark  recognition. OpenCV 2D feature extraction was used to to recognize landmarks  via  an  adaptive,  generic  system,  thereby avoiding  as  much  specialization-related  overhead  as possible. 
* SeaBee uses a specialized recognition algorithm for each unique landmark, excepting landmarks differing only in color. For  example,  when Seabee  isattempting  to  locate  a  buoy, itlooksfor orange pipelines and buoys of any color on approach, then looksfor buoys of a  single  color on each buoy-touching    attempt,    then    looks for only orange pipelines  and  yellow  hedges  as  it attempts to  locate the first hedge, etc. 
* Seabee’smost   advanced   sensor   is   an XSens  MTi  IMU.  
* Due to cost issues they were bound to  develop  both  a  generic realtime simulation of thevehicle's dynamics, as well as  a  generic  Bayesian  measurement  fusion  system capable    of    combining    all    components    of    all observables,    whether    simulated    or    actual,    into corresponding “filtered” measurements.
*  A realtime simulation  of  the  sub's  dynamicsis  runusing  a software   library   called   _BulletPhysics_.
# Univerity of Florida
SubjuGator  8’s  software  stack  (Figure 12)  is  built  on  the  Robot  Operating  System (ROS)   Indigo. 
* The   thruster   mapper   is   a   ROS   node responsible  for  translating  a  wrench (force and   moment) onto   an   arbitrary   set   of thrusters    using    a    box-constrained    least squares  solver.
* The   state   estimator   uses   an   inertial navigation   system   (INS)   and   an   indirect (error-state)  extended  Kalman  filter  (Figure 13). The INS integrates inertial measurements  from  the  IMU,producing  an orientation,  velocity,  and  position.  Due  to noise  and  unmodeled  errors  in  the  inertial sensors,  the  INS  rapidly  accumulates  error. 
* The  Kalman  filter  estimates  this  error  by comparing the output of the  INS against the reference sensors, which are a magnetometer,  depth  sensor,  and Doppler Velocity  Log(DVL). 
* SubjuGator  8  uses  a  novel  approach  for robustly  finding  objects  invarying  lighting and  water  conditions  using  a  particle  filter. Each  particle  in  the  filter  represents  a  guess of   where   the   object   might   be   in   three-dimensional    space,    as    well    as    what orientation  the  object  might  have.  The  filter then produces a template image of that guess by   taking   a   model   of   the   object   and rendering  it  as  it  would  be  seen  by  the camera.  The  template  is  compared  to  the actual  image  received  by  the  camera  and given  a  score.  Then,  high  scoring  particles are  reproduced  while  low  scoring  particles are   removed,   and   when   all   the   particles become   close   together,   the   filter   has   an accurate   estimate  of  where  the  object  is located. THis approach is superior as it is blind to environmental influence on colour, requires no thresholding, and gives a 3D pose of the object.
* In  addition  to  being  able  to  predict  the pose   of   objects   of   interests   in   three-dimensional, Subjugator  8  employs  the  use of  a  stereo  camera  system  to  further  check these   estimates
* Stimulating SubjuGator 8 entailed moving the entire  simulation  stack  to  Gazebo.  Gazebo allowed   the   team   to   tie   into   an   already existing    OpenGL    graphics    system    to visualize  the  simulations,  while also  providing  physics  simulations  via  the ODE  and  Bullet  physics  engines. 
This allowed  to  not  only  emulate  the  protocols  of  the various   hardware   devices   found   on   the AUV,  but  to  also  define  vehicle  dynamics, change  physics  properties,  modify  lighting on the fly, and also simulate sensor noise.
# University of Colorado Boulder
* The software system is designed to utilize ROS. The   framework   is   designed   around   a   central   mission planner,  which  takes  in  input  as  a  subscriber  from  different ROS  packages  and  then  outputs  a  desired  location  to  our motor control package. 
* The   localization   is   based   on   a   data   input   from   serial communication from the DVL and the IMU’s. The DVL takes care  of  both  velocity  and  position  outputs  which  can  be compared  to  one  another  to  verify  accuracy.  The  IMU’s  are sent   through   a   Kalman   filter   to   average   out   errors   and determine  the  vehicles  orientation.  Taking  the  data  from  each IMU  and  averaging  it  allows  us  to get  even  more  accurate estimates of our orientation.
 They   used OpenCV  to  identify  objects  such  as  buoys  and  to  prepare headings  based  on  the  underwater  guides. 
 * There  are  two cameras  being  used  on  the vehicle.The  first  is  a  forward facing  camera,  an  OccamRoboticsOmni60.  The  second  is  a downward  facing  cam, a  Point  Grey  Blackfly.  These  will  be implemented   with   three   main   ROS   packages.   The   first packages, DownCam and Occam will read in video data from our machine  vision  cameras,  and  translate  the  image  into  a ROS  mat  message  in  the  OpenCV  mat  image  format.  Then this   message   will   be   published   for   the   machine   vision package. Ourmachine visionpackage will accept image from both  the  downward  and  front  facing camera  and  identify objects   using   blob   detection   algorithms,   and   a   Hough transform  to  identify  paths  depending  on  the  camera,  and current stage of the mission planner.
 # University of Central Florida
 
