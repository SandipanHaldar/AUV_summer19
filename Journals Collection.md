# Washington State University
A simulation  platform  wasdeveloped  using  the  Unity  Engine  for  verification  testing  toensure functionality before time sensitive testing within a realpool.
The bot used  four  depth  sensors,  four  IMUs  each  con-sisting of a 3-axis gyroscope, accelerometer, and magnetometer, four hydrophones, two front-facing stereoscopic cameras,one  bottom-facing  camera,  and  current,  voltage,  and  rpmsensors  embedded  in  each  of  the  8  thrusters.  All  sensor  datais  integrated  using  an  Unscented  Kalman  Filter  (UKF),  with a particle  filter  acting  as  an  intermediary  for  the  hydrophone and camera data.
Gyroscopes, Depth sensors, Embedded Thruster sensor, Accelerometer, Magnetometer are notable instruments used. 
The magnetometers targeted IIR notch filters have been ex-perimented with successfully to remove oscillations, a simpleFIR Low pass filter is sufficient, and provides a consistent timedelay  which  is  more  preferable. 
Now comes the stimulator part
* Sensors- Hardware  sensors  were  not  simulated  directly due to their complexity, but their outputs were matched using Unity’s  built  in  3D  transforms.  Every  loop,  the  simulatedsensor module calculates the orientation of the submarine and outputs it to the software for processing. Noise can be added to  more  closely  simulate  bad  sensor  readings  depending  on observed noise within the real system.
* Thrusters:Thrusters  were  also  not  simulated  directly since fluid dynamics are too computationally expensive. However,  since  thrusters  were  linearized  in  the  real  system,  it  is assumed linear scaling of thruster values to the engine, making thruster simulation trivial. The thrusters are treated as a list of thrust  vectors  loaded  from  the  submarine  configuration  file.This  loads  the  offsets  of  the  thrusters  to  the  center  of  mass,their  orientations,  and  their  minimum  and  maximum  thrust.
* Cameras:Cameras were also not simulated directly due to  the  complexities  of  fish-eye  projection  and  lens  distortion.Fish-eye projection is also not supported in OpenGL or Unityand  must  be  done  manually  using  multiple  rectilinear  view-ports.  To  simplify,  since  cameras  are  calibrated,  undistorted,and  rectified  for  the  actual  system,  the  extrinsics  can  beloaded  to  place  cameras  in  the  same  location  and  projectionspace as the real submarine to recover the rectified coordinateframe. The vision pipeline then becomes identical as the realone barring re-projection errors within the original calibrationroutine.
* Submarine:Simulating the actual physics of the submarine was done much more directly than any other portion of the simulation. Since the Unity engine supports rigid body physics,mesh  rendering,  and  drag,  most  of  the  work  was  importing these  components  into  the  engine  using  approximations  from the original mechanical engineering designs. A visually representative rendering was done using a down-sampled mesh and texture  overlays  from  the  SolidWorks  model.  Statics  such  as the center of drag, mass, and buoyancy were also taken using SolidWorks  calculations  based  on  the  estimated  properties  of the submarine.  A simple  tensor  rotational/translational  drag  and  inertial  modelwas  made  that  could  closely  mimic  the  dynamics  of  the submarine in water without the complexities of fluid dynamics.
* Pool:Since the submarine can be tested in many different environments,  the  pool  simulation  is  just  as  important  as the  actual  submarine.  Pool  and  obstacle  simulation,  water reflections, fog rendering, lighting are all very complex problems  to  simulate  properly.  The  competition  pool  was  made using  Google  Earth  measurements  along  with  documented measurements of the geometry of the bottom. Obstacles were made  from  documents  as  well  and  can  be  placed  and  moved as needed once the final layout is measured at competition. All models were made using Google SketchUp and were imported to  the  engine  using  a  common  format.  Collision  meshes  and other scripted elements were generated once imported to allow entity interaction and debugging. To simulate water, the built-in  Unity  water  shaders  were  used  along  with  an  exponential fog  layer  in  the  camera  rendering  pipeline.  Lighting  is  also built  into  the  engine  and  can  be  adjusted  depending  the  timeof day and lighting conditions.  
# Utah State university
The microcomputer and microcontroller is based on a linux operating system
One Arduino Duo, a basic micro controller is present. 
Also utilizes four _Beaglebone Blacks_:microcontrollers similar to Raspberry Pi but more suited for robotics.
Raspberry Pi image capturing abilities were more user friendly so the replaced the three Beaglebones with one Raspberry Pi.
For vision they used opencv libary and coded in python.
Linux Debianis the ideal choice for Beaglebone Blacks due to an abundance of community support and available software packages. Linux Debian is also straightforward in installation and simple to use. 
# University if Victoria
* The  data from the  IMU and the  depth sensor are used to calculate  the  current orientation of the  vehicle. This data  is compared   to   the   location   of   the   target   object   and   a trajectory  to  the  target  object  is  calculated.  To  follow  this trajectory each motor needs to be set to a certain speed and the  computer  calculates  the  appropriate  speed  for  each motor and sends the commands.
* The computer of the AUV uses Linux mint running ROS.  ROS  nodes  and  nodelets  are used  to  control  the  sub. The  computer  sends  commands with an address for the specific peripheral. The junction box receives  this  command  and  reads  the  address  to  figure  out whom it is addressed to. It then converts the message to the right  protocol  for  the  intended  recipient  and  sends  the message to the peripheral. If the message is addressed to the junction  box  itself,  it  reads  the  message  and  carries  out  its commands.
Vision processing is done with OpenCV, which integrates with ROS. The vision is broken down into four parts
* Objects are recognised from a combination of edge detection and and colour recognition.   To  recognize  a specific  object,  a  classifier  is  created for  that  object  an trained to recognize the object by using several images with the object present as well as images of the background with the  object  not  present. A GUI was created to adjust the colour and edges for training the classifier.  The    classifier    also    does    not recognize an object when it is rotated, so the frames need to be  looked  at  four  times,  once  in  each  orientation,  to  see  if the classifier matches the object.
* Most of the  peripherals have  two set  of software. One  is embedded  code  on  a  microcontroller  to  control  the  actual operation  of  the  peripheral.  The  microcontroller  reads  the messages from the main computer and turns these messages into PWM signals that operate the devices. The peripherals get  their  instructions  from  the  task  routines  in  the  main control software.
# University of Toronto
Tempest’s software is organized in a Model-View-Controller (MVC) framework built upon the OpenCV and Qt libraries. Coded in C++, it is designed to be adaptable and user-friendly using Agile development principles. Tempest also includes a custom simulator to allow for virtual testing of the software algorithms. 
The  MVC  architecture  allows  for  more  modular code. The modularity decreases the learning curve for new  developersas  existing  code  can  be  treated  as black   boxes. 
The  task  algorithms  are  designed  to  control the movement   of   the   submarine.   They   receive   video feeds  from  the  front  and  bottom  cameras  as  well  as data from the FPGA. The video passes through a hue saturation  value  (HSV)  filter,  which  isolates  features that  are  within  preset  color  thresholds.  The  filtered video is then passed to other filters to undergo shape recognition.
The task algorithms use the geometry and position of  the  detected  shapes  as  well  as  the  data  from  the FPGA to execute motor commands. 
Tempest’s software testing system incorporates an in-house logger which allows for customizable output for  easy  debugging,  and  a  simulator  which  creates  a virtual environment to test the task algorithms.
The  logger  can  be  treated  as  an  augmented  print statement with     additional     features     such     as timestamps,   logging   levels,   and   writing   to   file. Logging also helps with code readability, which is an industry expectation.
The simulator is built using the 3D game engine Irrlicht. Irrlicht is chosen due to its    C++    libraries,    allowing    for    straightforward integration with the rest of the Tempest code. Irrlicht is  also  lightweight,  allowing  it  to  run  fairly  fast  on less powerful laptops.
